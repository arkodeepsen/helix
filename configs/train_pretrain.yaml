# Tokenization / data - Updated for BabyShark dataset
dataset_paths:
  - data/processed/babyshark_train
seq_len: 1024  # Reduce from 2048 to 1024 to save memory

# Optimization - Ultra memory optimized for DirectML AMD GPU
per_device_train_batch_size: 1  # Keep at 1
gradient_accumulation_steps: 128  # Increase further to maintain training effectiveness
learning_rate: 2.5e-4
weight_decay: 0.1
warmup_steps: 2000
total_steps: 25000
lr_scheduler_type: cosine
bf16: false  # DirectML works better with fp32
gradient_checkpointing: true
max_grad_norm: 1.0
logging_steps: 50
eval_steps: 500
save_steps: 500
save_total_limit: 5

# System - Optimized for Windows DirectML
num_workers: 0  # DirectML works better with 0 workers
device: privateuseone:0  # DirectML device
